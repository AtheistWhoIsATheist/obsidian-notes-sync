---
title: Models
created: 2025-08-06
last updated: 2025-08-06T05:55:40-07:00
description:
tags:
backlink: "[["
---
- [Mistral: Magistral Medium 2506](https://openrouter.ai/mistralai/magistral-medium-2506)
	[
	Magistral is Mistral's first reasoning model. It is ideal for general purpose use requiring longer thought processing and better accuracy than with non-reasoning LLMs. From legal research and financial forecasting to software development and creative storytelling — this model solves multi-step challenges where transparency and precision are critical.
	](https://openrouter.ai/mistralai/magistral-medium-2506)
	by [mistralai](https://openrouter.ai/mistralai) 41K context $2/M input $5/M output
- [Mistral: Magistral Medium 2506 (thinking)](https://openrouter.ai/mistralai/magistral-medium-2506:thinking)
	[
	Magistral is Mistral's first reasoning model. It is ideal for general purpose use requiring longer thought processing and better accuracy than with non-reasoning LLMs. From legal research and financial forecasting to software development and creative storytelling — this model solves multi-step challenges where transparency and precision are critical.
	](https://openrouter.ai/mistralai/magistral-medium-2506:thinking)
	by [mistralai](https://openrouter.ai/mistralai) 41K context $2/M input $5/M output
- [Google: Gemini 2.5 Pro Preview 06-05](https://openrouter.ai/google/gemini-2.5-pro-preview)
	[
	Gemini 2.5 Pro is Google’s state-of-the-art AI model designed for advanced reasoning, coding, mathematics, and scientific tasks. It employs “thinking” capabilities, enabling it to reason through responses with enhanced accuracy and nuanced context handling. Gemini 2.5 Pro achieves top-tier performance on multiple benchmarks, including first-place positioning on the LMArena leaderboard, reflecting superior human-preference alignment and complex problem-solving abilities.
	](https://openrouter.ai/google/gemini-2.5-pro-preview)
	by [google](https://openrouter.ai/google) 1.05M context $1.25/M input $10/M output
- [DeepSeek: R1 Distill Qwen 7B](https://openrouter.ai/deepseek/deepseek-r1-distill-qwen-7b)
	[
	DeepSeek-R1-Distill-Qwen-7B is a 7 billion parameter dense language model distilled from DeepSeek-R1, leveraging reinforcement learning-enhanced reasoning data generated by DeepSeek's larger models. The distillation process transfers advanced reasoning, math, and code capabilities into a smaller, more efficient model architecture based on Qwen2.5-Math-7B. This model demonstrates strong performance across mathematical benchmarks (92.8% pass@1 on MATH-500), coding tasks (Codeforces rating 1189), and general reasoning (49.1% pass@1 on GPQA Diamond), achieving competitive accuracy relative to larger models while maintaining smaller inference costs.
	](https://openrouter.ai/deepseek/deepseek-r1-distill-qwen-7b)
	by [deepseek](https://openrouter.ai/deepseek) 131K context $0.10/M input $0.20/M output
- [DeepSeek: Deepseek R1 0528 Qwen3 8B (free)](https://openrouter.ai/deepseek/deepseek-r1-0528-qwen3-8b:free)
	[
	DeepSeek-R1-0528 is a lightly upgraded release of DeepSeek R1 that taps more compute and smarter post-training tricks, pushing its reasoning and inference to the brink of flagship models like O3 and Gemini 2.5 Pro. It now tops math, programming, and logic leaderboards, showcasing a step-change in depth-of-thought. The distilled variant, DeepSeek-R1-0528-Qwen3-8B, transfers this chain-of-thought into an 8 B-parameter form, beating standard Qwen3 8B by +10 pp and tying the 235 B “thinking” giant on AIME 2024.
	](https://openrouter.ai/deepseek/deepseek-r1-0528-qwen3-8b:free)
	by [deepseek](https://openrouter.ai/deepseek) 131K context $0/M input $0/M output
- [DeepSeek: Deepseek R1 0528 Qwen3 8B](https://openrouter.ai/deepseek/deepseek-r1-0528-qwen3-8b)
	[
	DeepSeek-R1-0528 is a lightly upgraded release of DeepSeek R1 that taps more compute and smarter post-training tricks, pushing its reasoning and inference to the brink of flagship models like O3 and Gemini 2.5 Pro. It now tops math, programming, and logic leaderboards, showcasing a step-change in depth-of-thought. The distilled variant, DeepSeek-R1-0528-Qwen3-8B, transfers this chain-of-thought into an 8 B-parameter form, beating standard Qwen3 8B by +10 pp and tying the 235 B “thinking” giant on AIME 2024.
	](https://openrouter.ai/deepseek/deepseek-r1-0528-qwen3-8b)
	by [deepseek](https://openrouter.ai/deepseek) 32K context $0.01/M input $0.02/M output
- [DeepSeek: R1 0528 (free)](https://openrouter.ai/deepseek/deepseek-r1-0528:free)
	[
	May 28th update to the original DeepSeek R1 Performance on par with OpenAI o1, but open-sourced and with fully open reasoning tokens. It's 671B parameters in size, with 37B active in an inference pass. Fully open-source model.
	](https://openrouter.ai/deepseek/deepseek-r1-0528:free)
	by [deepseek](https://openrouter.ai/deepseek) 164K context $0/M input $0/M output
- [DeepSeek: R1 0528](https://openrouter.ai/deepseek/deepseek-r1-0528)
	[
	May 28th update to the original DeepSeek R1 Performance on par with OpenAI o1, but open-sourced and with fully open reasoning tokens. It's 671B parameters in size, with 37B active in an inference pass. Fully open-source model.
	](https://openrouter.ai/deepseek/deepseek-r1-0528)
	by [deepseek](https://openrouter.ai/deepseek) 164K context $0.18/M input $0.72/M output
- [Sarvam AI: Sarvam-M (free)](https://openrouter.ai/sarvamai/sarvam-m:free)
	[
	Sarvam-M is a 24 B-parameter, instruction-tuned derivative of Mistral-Small-3.1-24B-Base-2503, post-trained on English plus eleven major Indic languages (bn, hi, kn, gu, mr, ml, or, pa, ta, te). The model introduces a dual-mode interface: “non-think” for low-latency chat and a optional “think” phase that exposes chain-of-thought tokens for more demanding reasoning, math, and coding tasks. Benchmark reports show solid gains versus similarly sized open models on Indic-language QA, GSM-8K math, and SWE-Bench coding, making Sarvam-M a practical general-purpose choice for multilingual conversational agents as well as analytical workloads that mix English, native Indic scripts, or romanized text.
	](https://openrouter.ai/sarvamai/sarvam-m:free)
	by [sarvamai](https://openrouter.ai/sarvamai) 33K context $0/M input $0/M output
- [Anthropic: Claude Opus 4](https://openrouter.ai/anthropic/claude-opus-4)
	[
	Claude Opus 4 is benchmarked as the world’s best coding model, at time of release, bringing sustained performance on complex, long-running tasks and agent workflows. It sets new benchmarks in software engineering, achieving leading results on SWE-bench (72.5%) and Terminal-bench (43.2%). Opus 4 supports extended, agentic workflows, handling thousands of task steps continuously for hours without degradation. Read more at the blog post here
	](https://openrouter.ai/anthropic/claude-opus-4)
	by [anthropic](https://openrouter.ai/anthropic) 200K context $15/M input $75/M output
- [Anthropic: Claude Sonnet 4](https://openrouter.ai/anthropic/claude-sonnet-4)
	[
	Claude Sonnet 4 significantly enhances the capabilities of its predecessor, Sonnet 3.7, excelling in both coding and reasoning tasks with improved precision and controllability. Achieving state-of-the-art performance on SWE-bench (72.7%), Sonnet 4 balances capability and computational efficiency, making it suitable for a broad range of applications from routine coding tasks to complex software development projects. Key enhancements include improved autonomous codebase navigation, reduced error rates in agent-driven workflows, and increased reliability in following intricate instructions. Sonnet 4 is optimized for practical everyday use, providing advanced reasoning capabilities while maintaining efficiency and responsiveness in diverse internal and external scenarios. Read more at the blog post here
	](https://openrouter.ai/anthropic/claude-sonnet-4)
	by [anthropic](https://openrouter.ai/anthropic) 200K context $3/M input $15/M output
- [Mistral: Devstral Small 2505 (free)](https://openrouter.ai/mistralai/devstral-small-2505:free)
	[
	Devstral-Small-2505 is a 24B parameter agentic LLM fine-tuned from Mistral-Small-3.1, jointly developed by Mistral AI and All Hands AI for advanced software engineering tasks. It is optimized for codebase exploration, multi-file editing, and integration into coding agents, achieving state-of-the-art results on SWE-Bench Verified (46.8%). Devstral supports a 128k context window and uses a custom Tekken tokenizer. It is text-only, with the vision encoder removed, and is suitable for local deployment on high-end consumer hardware (e.g., RTX 4090, 32GB RAM Macs). Devstral is best used in agentic workflows via the OpenHands scaffold and is compatible with inference frameworks like vLLM, Transformers, and Ollama. It is released under the Apache 2.0 license.
	](https://openrouter.ai/mistralai/devstral-small-2505:free)
	by [mistralai](https://openrouter.ai/mistralai) 33K context $0/M input $0/M output
- [Mistral: Devstral Small 2505](https://openrouter.ai/mistralai/devstral-small-2505)
	[
	Devstral-Small-2505 is a 24B parameter agentic LLM fine-tuned from Mistral-Small-3.1, jointly developed by Mistral AI and All Hands AI for advanced software engineering tasks. It is optimized for codebase exploration, multi-file editing, and integration into coding agents, achieving state-of-the-art results on SWE-Bench Verified (46.8%). Devstral supports a 128k context window and uses a custom Tekken tokenizer. It is text-only, with the vision encoder removed, and is suitable for local deployment on high-end consumer hardware (e.g., RTX 4090, 32GB RAM Macs). Devstral is best used in agentic workflows via the OpenHands scaffold and is compatible with inference frameworks like vLLM, Transformers, and Ollama. It is released under the Apache 2.0 license.
	](https://openrouter.ai/mistralai/devstral-small-2505)
	by [mistralai](https://openrouter.ai/mistralai) 33K context $0.02/M input $0.08/M output

Models | OpenRouter